[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Qinyi(Chloe) Hu",
    "section": "",
    "text": "Welcome to my website!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "My Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "projects/project1/index.html",
    "href": "projects/project1/index.html",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Let’s investigate the relationship between fuel efficiency (mpg) and engine displacement (disp) from the mtcars dataset. Those variables have a correlation of -0.85.\n\n\nHere is a plot:\n\nlibrary(tidyverse)\ndata(mtcars)\nmtcars |&gt;\n  ggplot(aes(mpg, disp)) + \n  geom_point(color=\"dodgerblue4\", size=2)"
  },
  {
    "objectID": "projects/project1/index.html#sub-header",
    "href": "projects/project1/index.html#sub-header",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Here is a plot:\n\nlibrary(tidyverse)\ndata(mtcars)\nmtcars |&gt;\n  ggplot(aes(mpg, disp)) + \n  geom_point(color=\"dodgerblue4\", size=2)"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "Analysis of Cars\n\n\n\n\n\n\nChloe Hu\n\n\nApr 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Projects",
    "section": "",
    "text": "A Replication of Karlan and List (2007)\n\n\n\n\n\n\nQinyi Hu\n\n\nMay 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nQinyi Hu\n\n\nMay 3, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/project1/index.html",
    "href": "blog/project1/index.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Charitable giving constitutes a significant component of private contributions to public goods, with Americans donating consistently around 2% of GDP annually. Despite the scale and importance of these donations, fundraisers have historically relied on intuition and anecdotal evidence rather than rigorous experimental data to guide their strategies.\nDean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe standard letter asked for a donation in the usual way, without any mention of matching. This formed the control group. In contrast, both the matching grant letter and the challenge grant letter indicated that a fellow donor had agreed to match each contribution at a given rate, conditional on others also donating. While the language varied slightly between “matching” and “challenge” framing, both effectively served as treatment groups and were analyzed together in the main analysis.\nThe core question explored in this study was whether offering a match — and varying the generosity of that match — would affect donor behavior. The researchers were interested in how matching offers influence the extensive margin (whether people donate at all) and the intensive margin (how much they give).\nTo explore this, recipients were randomly assigned to fundraising letters that varied along three dimensions:\n\nMatch ratio: $1:$1, $2:$1, or $3:$1\nMaximum match amount: either $25,000, $50,000, $100,000, or left unspecified\nSuggested donation amount: tailored to each donor’s past behavior (equal to their highest previous gift, 1.25× that amount, or 1.5×)\n\nThe main outcome variables were whether a recipient donated and, if so, how much they contributed. Karlan and List found that simply including a matching offer — regardless of the exact match ratio — increased both the probability of giving and the revenue raised per letter. However, increasing the match ratio from $1:$1 to $2:$1 or $3:$1 yielded no additional benefit, suggesting diminishing or flat marginal returns from more generous matching. Additionally, the effect was stronger in red states (those that voted for George W. Bush in 2004), pointing to interesting heterogeneity in treatment effects across political environments.\nThis project seeks to replicate their results. Using the original dataset, we examine how the availability and generosity of matching gifts influence giving behavior. Along the way, we visualize treatment effects and estimate regression models similar to those in the published paper. The goal is not only to test the robustness of their results but also to understand what types of interventions are most effective in motivating donors."
  },
  {
    "objectID": "blog/project1/index.html#introduction",
    "href": "blog/project1/index.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Charitable giving constitutes a significant component of private contributions to public goods, with Americans donating consistently around 2% of GDP annually. Despite the scale and importance of these donations, fundraisers have historically relied on intuition and anecdotal evidence rather than rigorous experimental data to guide their strategies.\nDean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe standard letter asked for a donation in the usual way, without any mention of matching. This formed the control group. In contrast, both the matching grant letter and the challenge grant letter indicated that a fellow donor had agreed to match each contribution at a given rate, conditional on others also donating. While the language varied slightly between “matching” and “challenge” framing, both effectively served as treatment groups and were analyzed together in the main analysis.\nThe core question explored in this study was whether offering a match — and varying the generosity of that match — would affect donor behavior. The researchers were interested in how matching offers influence the extensive margin (whether people donate at all) and the intensive margin (how much they give).\nTo explore this, recipients were randomly assigned to fundraising letters that varied along three dimensions:\n\nMatch ratio: $1:$1, $2:$1, or $3:$1\nMaximum match amount: either $25,000, $50,000, $100,000, or left unspecified\nSuggested donation amount: tailored to each donor’s past behavior (equal to their highest previous gift, 1.25× that amount, or 1.5×)\n\nThe main outcome variables were whether a recipient donated and, if so, how much they contributed. Karlan and List found that simply including a matching offer — regardless of the exact match ratio — increased both the probability of giving and the revenue raised per letter. However, increasing the match ratio from $1:$1 to $2:$1 or $3:$1 yielded no additional benefit, suggesting diminishing or flat marginal returns from more generous matching. Additionally, the effect was stronger in red states (those that voted for George W. Bush in 2004), pointing to interesting heterogeneity in treatment effects across political environments.\nThis project seeks to replicate their results. Using the original dataset, we examine how the availability and generosity of matching gifts influence giving behavior. Along the way, we visualize treatment effects and estimate regression models similar to those in the published paper. The goal is not only to test the robustness of their results but also to understand what types of interventions are most effective in motivating donors."
  },
  {
    "objectID": "blog/project1/index.html#data",
    "href": "blog/project1/index.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\nThe dataset used in this project comes from the original field experiment conducted by Karlan and List (2007). It contains information on 50,083 prior donors who were randomly assigned to receive one of several versions of a fundraising letter. Each row in the dataset represents a single individual, and the columns record treatment assignments, past donation behavior, observed giving outcomes, and various demographic and geographic characteristics.\n\nimport pandas as pd\ndf = pd.read_stata(\"karlan_list_2007.dta\")\ndf.head()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.0\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.0\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.0\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.0\n\n\n\n\n5 rows × 51 columns\n\n\n\nThe data includes several important variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\nBelow, we summarize the dataset:\n\ndf[['gave', 'amount', 'hpa', 'freq', 'years']].describe()\n\n\n\n\n\n\n\n\ngave\namount\nhpa\nfreq\nyears\n\n\n\n\ncount\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50082.000000\n\n\nmean\n0.020646\n0.915694\n59.384975\n8.039355\n6.097540\n\n\nstd\n0.142197\n8.707393\n71.179871\n11.394454\n5.503492\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\n30.000000\n2.000000\n2.000000\n\n\n50%\n0.000000\n0.000000\n45.000000\n4.000000\n5.000000\n\n\n75%\n0.000000\n0.000000\n60.000000\n10.000000\n9.000000\n\n\nmax\n1.000000\n400.000000\n1000.000000\n218.000000\n95.000000\n\n\n\n\n\n\n\n\ndf['treatment'].value_counts()\n\n1    33396\n0    16687\nName: treatment, dtype: int64\n\n\n\ndf[['treatment', 'gave', 'amount', 'hpa']].isnull().sum()\n\ntreatment    0\ngave         0\namount       0\nhpa          0\ndtype: int64\n\n\nWe observe that treatment and control groups are appropriately split, with no major issues related to missing values in the core outcome variables. This ensures a clean setup for replicating the main analysis from the original paper.\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I compare baseline characteristics between the treatment and control groups. If the randomization was successful, there should be no statistically significant differences in these pre-treatment covariates. To test this, I examine selected variables using both two-sample t-tests and linear regressions, and confirm that both methods yield equivalent results.\nBelow, I analyze the variable mrm2 — the number of months since the individual’s last donation — as a representative pre-treatment covariate. I also test additional variables such as hpa (highest previous contribution) and years (years since first donation). These variables are all unrelated to the treatment assignment and therefore should be balanced across groups.\n1. Balance Test for mrm2 (Months Since Last Donation)\nWe first compute the t-statistic manually:\n\n# Select values and drop missing observations\ntreat = df[df['treatment'] == 1]['mrm2'].dropna()\ncontrol = df[df['control'] == 1]['mrm2'].dropna()\n\n# Means, variances, sample sizes\nx1_bar, x0_bar = treat.mean(), control.mean()\ns1_sq, s0_sq = treat.var(ddof=1), control.var(ddof=1)\nn1, n0 = treat.shape[0], control.shape[0]\n\n# Manual t-statistic\nt_stat = (x1_bar - x0_bar) / ((s1_sq / n1 + s0_sq / n0) ** 0.5)\nt_stat\n\n0.11953155228176904\n\n\nNow we use a regression to estimate the same difference:\n\n# Regression of months since last donation on treatment\n# Intercept = control group mean; treatment coefficient = difference\nimport statsmodels.formula.api as smf\n\nmodel_mrm2 = smf.ols('mrm2 ~ treatment', data=df).fit()\nmodel_mrm2.summary2().tables[1]\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\n\n\nIntercept\n12.998142\n0.093526\n138.978873\n0.000000\n12.814830\n13.181454\n\n\ntreatment\n0.013686\n0.114534\n0.119492\n0.904886\n-0.210801\n0.238173\n\n\n\n\n\n\n\n2. Balance Test for hpa (Highest Previous Contribution)\n\ntreat = df[df['treatment'] == 1]['hpa'].dropna()\ncontrol = df[df['control'] == 1]['hpa'].dropna()\n\nx1_bar, x0_bar = treat.mean(), control.mean()\ns1_sq, s0_sq = treat.var(ddof=1), control.var(ddof=1)\nn1, n0 = treat.shape[0], control.shape[0]\n\nt_stat = (x1_bar - x0_bar) / ((s1_sq / n1 + s0_sq / n0) ** 0.5)\nt_stat\n\n0.9703896722043864\n\n\n\nmodel_hpa = smf.ols('hpa ~ treatment', data=df).fit()\nmodel_hpa.summary2().tables[1]\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\n\n\nIntercept\n58.960167\n0.551002\n107.005425\n0.000000\n57.880198\n60.040137\n\n\ntreatment\n0.637075\n0.674762\n0.944148\n0.345099\n-0.685467\n1.959617\n\n\n\n\n\n\n\n3. Balance Test for years (Years Since Initial Donation)\n\ntreat = df[df['treatment'] == 1]['years'].dropna()\ncontrol = df[df['control'] == 1]['years'].dropna()\n\nx1_bar, x0_bar = treat.mean(), control.mean()\ns1_sq, s0_sq = treat.var(ddof=1), control.var(ddof=1)\nn1, n0 = treat.shape[0], control.shape[0]\n\nt_stat = (x1_bar - x0_bar) / ((s1_sq / n1 + s0_sq / n0) ** 0.5)\nt_stat\n\n-1.0909175279573782\n\n\n\nmodel_years = smf.ols('years ~ treatment', data=df).fit()\nmodel_years.summary2().tables[1]\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\n\n\nIntercept\n6.135914\n0.042604\n144.022723\n0.000000\n6.052410\n6.219418\n\n\ntreatment\n-0.057549\n0.052173\n-1.103038\n0.270016\n-0.159809\n0.044711\n\n\n\n\n\n\n\nFor all three variables, both testing methods yielded consistent results: there were no statistically significant differences between the groups. The estimated differences were small, the t-statistics did not exceed standard critical values, and p-values were well above the conventional 0.05 level. The regression coefficients on the treatment variable mirrored the difference-in-means estimates, confirming the equivalence of the two methods.\nThese findings are consistent with Table 1 from Karlan and List (2007), which shows that the means of these same variables are nearly identical across treatment and control groups. For example:\n\nmrm2: 13.012 (treatment) vs. 12.998 (control)\nhpa: 59.60 (treatment) vs. 58.96 (control)\nyears: 6.08 (treatment) vs. 6.14 (control)\n\nNone of these differences are large enough to raise concerns about imbalance.\nTable 1 serves a critical purpose in field experiment papers: it reassures the reader that the random assignment was implemented correctly. In a randomized controlled trial (RCT), we expect pre-treatment characteristics to be balanced across groups. If large imbalances were found, this might indicate flaws in the randomization or possible selection bias. Since Table 1 shows no such imbalances, it supports the internal validity of the authors’ causal estimates."
  },
  {
    "objectID": "blog/project1/index.html#experimental-results",
    "href": "blog/project1/index.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation. The chart below shows the proportion of individuals who gave a donation in each group.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Calculate donation rate (mean of 'gave') for treatment and control\ngroup_means = df.groupby(\"treatment\")[\"gave\"].mean().reset_index()\ngroup_means[\"Group\"] = group_means[\"treatment\"].replace({1: \"Treatment\", 0: \"Control\"})\n\n# Create bar plot\nplt.figure(figsize=(6, 4))\nbars = plt.bar(group_means[\"Group\"], group_means[\"gave\"], color=[\"#1f77b4\", \"#ff7f0e\"])\nplt.ylabel(\"Proportion Who Donated\")\nplt.ylim(0, 0.03)\nplt.title(\"Donation Rate by Treatment Group\")\nplt.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\nfor bar in bars:\n    height = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width()/2, height + 0.001, f\"{height:.3%}\", \n             ha='center', va='bottom', fontsize=10)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nDonation rates in treatment and control groups\n\n\n\n\nWe observe a clear difference in the donation rates: the treatment group had a higher proportion of donors than the control group. This aligns with the findings of Karlan and List (2007), who report that simply mentioning a matching grant significantly increases the probability of giving.\nIn our plot, the donation rate for the control group is approximately 1.8% while that for the treatment group is around 2.2%. This simple descriptive comparison already suggests a positive impact of matching on charitable behavior, which we will further test using other methods.\nWe now test whether receiving a matching donation offer increases the likelihood of making any donation. Specifically, we compare the response rate (gave == 1) between the treatment and control groups using t-test and linear regression.\n\ntreat = df[df['treatment'] == 1]['gave'].dropna()\ncontrol = df[df['control'] == 1]['gave'].dropna()\n\nx1_bar, x0_bar = treat.mean(), control.mean()\ns1_sq, s0_sq = treat.var(ddof=1), control.var(ddof=1)\nn1, n0 = treat.shape[0], control.shape[0]\n\nt_stat = (x1_bar - x0_bar) / ((s1_sq / n1 + s0_sq / n0) ** 0.5)\nt_stat\n\n3.2094621908279835\n\n\n\nmodel = smf.ols('gave ~ treatment', data=df).fit()\nmodel.summary2().tables[1]\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\n\n\nIntercept\n0.017858\n0.001101\n16.224643\n4.779032e-59\n0.015701\n0.020016\n\n\ntreatment\n0.004180\n0.001348\n3.101361\n1.927403e-03\n0.001538\n0.006822\n\n\n\n\n\n\n\nThe results show that people who received a treatment letter (i.e., a letter offering a matching donation) were significantly more likely to make a donation compared to those in the control group. The t-test and the regression both confirm that this difference is statistically significant.\nIn the regression, the intercept represents the control group’s donation rate (around 1.8%), and the coefficient on treatment shows the additional increase for the treatment group (roughly +0.4 percentage points). This matches the results in Table 2A, Panel A of the original paper (1.8% vs 2.2%).\nSo what does this result tell us about human behavior?\nSimply mentioning a match — even without changing the suggested amount — makes people more likely to give. This shows that social framing and perceived impact play a strong role in motivating charitable actions. People are more willing to give when they believe their donation is being “amplified” by a matching donor.\nThis insight is valuable for fundraisers: offering a match — even at a 1:1 ratio — is a low-cost but effective behavioral nudge to increase participation.\nWe now estimate a Probit model where the dependent variable is whether an individual donated (gave) and the independent variable is whether they received a matching grant offer (treatment).\n\nprobit_model = smf.probit(\"gave ~ treatment\", data=df).fit()\nprobit_model.summary2().tables[1]\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\n\n\nIntercept\n-2.100141\n0.023316\n-90.07277\n0.000000\n-2.145840\n-2.054443\n\n\ntreatment\n0.086785\n0.027879\n3.11293\n0.001852\n0.032143\n0.141426\n\n\n\n\n\n\n\nThe Probit regression shows that being assigned to the treatment group has a positive and statistically significant effect on the likelihood of donating. The estimated coefficient is 0.087, with a p-value &lt; 0.01, indicating strong evidence against the null hypothesis.\nThen we calculate the marginal effects:\n\nmfx = probit_model.get_margeff()\nmfx.summary()\n\n\nProbit Marginal Effects\n\n\nDep. Variable:\ngave\n\n\nMethod:\ndydx\n\n\nAt:\noverall\n\n\n\n\n\n\n\n\n\ndy/dx\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\ntreatment\n0.0043\n0.001\n3.104\n0.002\n0.002\n0.007\n\n\n\n\n\nWhen we compute the marginal effect, which represents the actual increase in donation probability, we obtain a value around 0.004, which exactly matches the reported result in Table 3, Column (1) of the paper. This means that being offered a matching gift increased the chance of donating by about 0.4 percentage points.\nAlthough this effect might sound small, it is statistically meaningful given the large sample size and low baseline donation rate (~1.8%). This reinforces the behavioral insight that even small framing changes, such as introducing a match, can have a measurable influence on charitable behavior.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different match ratios on the probability of donating. Specifically, I compare the response rate across $1:$1, $2:$1, and $3:$1 match treatments using two-sample t-tests.\n\nfrom scipy import stats\n\n# Filter only treatment group\ndf_treat = df[df['treatment'] == 1]\n\n# Split by match ratio\ngave_1to1 = df_treat[df_treat['ratio'] == 1]['gave'].dropna()\ngave_2to1 = df_treat[df_treat['ratio'] == 2]['gave'].dropna()\ngave_3to1 = df_treat[df_treat['ratio'] == 3]['gave'].dropna()\n\n# 1:1 vs 2:1\ntstat_12, pval_12 = stats.ttest_ind(gave_1to1, gave_2to1, equal_var=True)\n\n# 1:1 vs 3:1\ntstat_13, pval_13 = stats.ttest_ind(gave_1to1, gave_3to1, equal_var=True)\n\n# 2:1 vs 3:1\ntstat_23, pval_23 = stats.ttest_ind(gave_2to1, gave_3to1, equal_var=True)\n\n{\n    \"1:1 vs 2:1\": (tstat_12, pval_12),\n    \"1:1 vs 3:1\": (tstat_13, pval_13),\n    \"2:1 vs 3:1\": (tstat_23, pval_23)\n}\n\n{'1:1 vs 2:1': (-0.96504713432247, 0.33453168549723933),\n '1:1 vs 3:1': (-1.0150255853798622, 0.3101046637086672),\n '2:1 vs 3:1': (-0.05011583793874515, 0.9600305283739325)}\n\n\nAll three t-tests produce p-values well above 0.05, suggesting that the differences in donation rates between match ratios are not statistically significant. That is, offering a $2:$1 or $3:$1 match did not lead to higher likelihood of giving compared to a $1:$1 match.\nThis result replicates the comment the authors make on page 8, where the authors note that “gift distributions across the various matching ratios are not significantly different from one another.” The behavioral implication is striking: people seem to respond to the existence of a match, but not to how generous it is. This challenges the conventional wisdom in fundraising, which assumes that more generous matching offers will spur higher response rates.\nFrom a policy or fundraising perspective, this suggests that a simple 1:1 match may be just as effective as a more expensive 3:1 match, at least in motivating participation.\nWe now manually create dummy variables for each match ratio level and run a linear regression to test whether more generous match ratios affect the likelihood of donating.\n\n# Filter treatment group only\ndf_treat = df[df['treatment'] == 1].copy()\n\n# Convert match ratio to dummy variables\ndf_treat['ratio1'] = (df_treat['ratio'] == 1).astype(int)\ndf_treat['ratio2'] = (df_treat['ratio'] == 2).astype(int)\ndf_treat['ratio3'] = (df_treat['ratio'] == 3).astype(int)\n\n# Omit ratio1 to serve as baseline\nimport statsmodels.formula.api as smf\n\nmodel_dummy = smf.ols('gave ~ ratio2 + ratio3', data=df_treat).fit()\nmodel_dummy.summary2().tables[1]\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\n\n\nIntercept\n0.020749\n0.001391\n14.912217\n3.981333e-50\n0.018022\n0.023476\n\n\nratio2\n0.001884\n0.001968\n0.957582\n3.382805e-01\n-0.001973\n0.005741\n\n\nratio3\n0.001984\n0.001968\n1.008301\n3.133172e-01\n-0.001873\n0.005842\n\n\n\n\n\n\n\nThe regression uses $1:$1 as the reference group. The intercept (0.0207) represents the estimated donation rate under a $1:$1 match — about 2.07% of people in this group made a donation.\nThe coefficients on ratio2 and ratio3 show the change in donation rate when the match ratio is increased to $2:$1 and $3:$1, respectively:\n\nThe $2:$1 match increases the donation rate by only 0.19 percentage points.\nThe $3:$1 match increases it by a similar amount (0.20 percentage points)\n\nHowever, these coefficients are not statistically significant:\n\nThe p-values for both are above 0.3\nTheir 95% confidence intervals include zero\n\nThis means we cannot rule out the possibility that these differences happened by chance. In short, increasing the generosity of the match ratio did not significantly increase the likelihood of donating.\nThese findings are entirely consistent with the paper’s conclusion on page 8: “the gift distributions across the various matching ratios are not significantly different from one another.” This challenges the common assumption in fundraising that more generous matches (e.g., $3:$1) are always better than simpler ones like $1:$1. It appears that the presence of a match is what matters — not necessarily how large it is.\nWe now directly compute the difference in donation rates across match ratios — both from the raw data and from the fitted regression coefficients.\n\n# Raw response rates\nrate_1to1 = df_treat[df_treat['ratio'] == 1]['gave'].mean()\nrate_2to1 = df_treat[df_treat['ratio'] == 2]['gave'].mean()\nrate_3to1 = df_treat[df_treat['ratio'] == 3]['gave'].mean()\n\n# Differences from data\ndiff_21_data = rate_2to1 - rate_1to1\ndiff_32_data = rate_3to1 - rate_2to1\n\n# Differences from regression coefficients\ncoef_21 = model_dummy.params['ratio2']\ncoef_31 = model_dummy.params['ratio3']\ndiff_21_reg = coef_21\ndiff_32_reg = coef_31 - coef_21\n\n{\n    \"From data: 2:1 - 1:1\": diff_21_data,\n    \"From data: 3:1 - 2:1\": diff_32_data,\n    \"From regression: 2:1 - 1:1\": diff_21_reg,\n    \"From regression: 3:1 - 2:1\": diff_32_reg\n}\n\n{'From data: 2:1 - 1:1': 0.0018842510217149944,\n 'From data: 3:1 - 2:1': 0.00010002398025293902,\n 'From regression: 2:1 - 1:1': 0.0018842510217150174,\n 'From regression: 3:1 - 2:1': 0.00010002398025293815}\n\n\nThe results from both the raw data and regression model tell a consistent story:\n\nThe difference in donation rate between $1:$1 and $2:$1 is very small — less than 0.2 percentage points\nThe difference between $2:$1 and $3:$1 is even smaller, and in some versions may even be slightly negative\n\nFrom the regression, we see that the estimated increase from $1:$1 to $2:$1 is about 0.19 percentage points, while the additional increase from $2:$1 to $3:$1 is effectively zero.\nThese differences are also statistically insignificant, with large p-values and confidence intervals that include zero.\nWe can conclude that increasing the generosity of the match ratio — from $1:$1 to $2:$1 or $3:$1 — does not meaningfully change donation behavior. What seems to matter is simply the presence of a match, not how large it is.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of being offered a matching donation on the size of the donation. Specifically, I compare average donation amounts between treatment and control groups.\nWe begin with a t-test and then verify the result using a simple linear regression.\n\n# Drop missing values\namount_treat = df[df['treatment'] == 1]['amount'].dropna()\namount_control = df[df['control'] == 1]['amount'].dropna()\n\n# T-test\ntstat_amt, pval_amt = stats.ttest_ind(amount_treat, amount_control, equal_var=True)\ntstat_amt, pval_amt\n\n(1.8605020225753781, 0.06282038947470686)\n\n\n\nmodel_amount = smf.ols(\"amount ~ treatment\", data=df).fit()\nmodel_amount.summary2().tables[1]\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\n\n\nIntercept\n0.813268\n0.067418\n12.062995\n1.843438e-33\n0.681127\n0.945409\n\n\ntreatment\n0.153605\n0.082561\n1.860503\n6.282029e-02\n-0.008216\n0.315426\n\n\n\n\n\n\n\nThe intercept (≈ $0.81) represents the average amount given in the control group. The coefficient on treatment (≈ $0.15) shows that, on average, those in the treatment group gave 15 cents more.\nHowever, this difference is only marginally significant because the p-value from the regression is ≈ 0.063 which is just above the conventional 5% threshold and the 95% confidence interval includes zero (−0.008 to +0.315).\nThe same conclusion emerges from the t-test.\nThis suggests that being offered a matching donation may increase the total donation amount, but the evidence is not very strong. It supports the idea that treatment has a directional positive effect, but we cannot rule out chance as an explanation.\nMoreover, this is the unconditional amount — it includes many people who gave $0. In other words, the increase might be driven by more people giving, rather than each person giving significantly more. This points us toward analyzing the conditional giving effect (among those who donated), which we’ll explore next.\nWe now restrict the analysis to only those who made a donation (i.e., gave == 1). This allows us to assess how treatment affects the amount given, conditional on a donation being made.\n\n# Filter only those who gave\ndf_donated = df[df['gave'] == 1].copy()\n\n# Run OLS regression: amount ~ treatment\nimport statsmodels.formula.api as smf\nmodel_cond = smf.ols(\"amount ~ treatment\", data=df_donated).fit()\nmodel_cond.summary2().tables[1]\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\n\n\nIntercept\n45.540268\n2.423378\n18.792063\n5.473578e-68\n40.784958\n50.295579\n\n\ntreatment\n-1.668393\n2.872384\n-0.580839\n5.614756e-01\n-7.304773\n3.967986\n\n\n\n\n\n\n\nThis regression analyzes how much individuals donated, conditional on making a donation. We restrict the sample to gave == 1, and regress donation amount (amount) on treatment status.\n\nThe intercept (≈ $45.54) is the average donation amount among control group donors. This aligns exactly with Table 2A Panel A.\nThe treatment coefficient is −$1.67, with a p-value of 0.56 — it is small and statistically insignificant.\n\nThis suggests that, among people who chose to donate receiving a matching letter did not significantly change how much they gave. In fact, the (insignificant) point estimate is slightly negative, perhaps because treatment attracted more “marginal” donors (who might give smaller amounts).\nThis is not a fully causal estimate of how treatment affects donation size, because we’re conditioning on a post-treatment variable (gave == 1), which may introduce selection bias.\nThe treatment may cause more marginal givers to donate (who tend to give less) and this could bias the treatment effect downward in this conditional sample.\nTherefore, this estimate does not cleanly identify a causal effect of treatment on giving amount. The unconditional effect is the more policy-relevant and causally interpretable one.\nTo better understand the distribution of donation amounts, I plot histograms of donation sizes only among those who gave, separated by treatment status. Each plot includes a red dashed line to indicate the average donation in that group.\n\nimport matplotlib.pyplot as plt\n\n# Filter to only people who gave\ndf_donated = df[df['gave'] == 1].copy()\ndf_treat = df_donated[df_donated['treatment'] == 1]\ndf_control = df_donated[df_donated['control'] == 1]\n\n# Calculate group means\nmean_treat = df_treat['amount'].mean()\nmean_control = df_control['amount'].mean()\n\n# Plot setup\nfig, axs = plt.subplots(2, 1, figsize=(6, 7), sharey=True)\n\n# Treatment group histogram\naxs[0].hist(df_treat['amount'], bins=30, color='#1f77b4', edgecolor='white')\naxs[0].axvline(mean_treat, color='red', linestyle='--', linewidth=2)\naxs[0].set_title('Treatment Group')\naxs[0].set_xlabel('Donation Amount')\naxs[0].set_ylabel('Number of Donors')\naxs[0].text(mean_treat + 1, axs[0].get_ylim()[1]*0.9, f'Mean = {mean_treat:.2f}', color='red')\n\n# Control group histogram\naxs[1].hist(df_control['amount'], bins=30, color='#ff7f0e', edgecolor='white')\naxs[1].axvline(mean_control, color='red', linestyle='--', linewidth=2)\naxs[1].set_title('Control Group')\naxs[1].set_xlabel('Donation Amount')\naxs[1].text(mean_control + 1, axs[1].get_ylim()[1]*0.9, f'Mean = {mean_control:.2f}', color='red')\n\nplt.suptitle(\"Donation Amounts Among Donors\")\nplt.tight_layout()\nplt.show()\n\n\n\n\nHistogram of Donation Amounts by Treatment Group (among donors)\n\n\n\n\nThese histograms show the distribution of donation amounts among those who donated, separated by treatment status.\nThe red dashed line in each plot represents the average donation in that group. The control group has a mean of about $45.54, and the treatment group has a slightly lower mean (≈ $43.87), matching the regression results from earlier.\nBoth distributions are right-skewed, which means a few large donations pull the average up. Most people gave less than $50, and the treatment group appears to have more small donations, which may explain the slightly lower average.\nThis visualization confirms the finding that while the likelihood of giving increases with treatment, the amount given (conditional on giving) does not."
  },
  {
    "objectID": "blog/project1/index.html#simulation-experiment",
    "href": "blog/project1/index.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nWe then calculate the cumulative average of the difference in simulated outcomes across 10,000 draws and plot how this converges to the true difference of 0.004.\n\nimport numpy as np\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Simulate donation outcomes\ncontrol_draws = np.random.binomial(n=1, p=0.018, size=100000)\ntreat_draws = np.random.binomial(n=1, p=0.022, size=10000)\n\n# Match lengths (take first 10,000 of control to align)\ncontrol_sample = control_draws[:10000]\n\n# Compute difference at each observation\ndiffs = treat_draws - control_sample\n\n# Cumulative average of difference\ncum_avg = np.cumsum(diffs) / np.arange(1, len(diffs) + 1)\n\nplt.figure(figsize=(8, 5))\nplt.plot(cum_avg, label='Cumulative average difference')\nplt.axhline(0.004, color='red', linestyle='--', label='True difference (0.004)')\nplt.xlabel(\"Number of observations\")\nplt.ylabel(\"Cumulative average of difference\")\nplt.title(\"Demonstration of Law of Large Numbers\")\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.tight_layout()\nplt.show()\n\n\n\n\nCumulative Average of Difference in Simulated Donation Outcomes\n\n\n\n\nThe plot shows how the cumulative average difference in donation rates between simulated treatment and control groups evolves as the sample size increases.\nAt the beginning (small n), the average is highly volatile — due to sampling noise. As more observations are added, the average stabilizes and converges to the true difference (0.004). The red dashed line represents the true gap in donation rates between the two groups.\nThis is a visual demonstration of the Law of Large Numbers: with enough independent observations, the sample average converges to the population mean.\n\n\nCentral Limit Theorem\nTo demonstrate the Central Limit Theorem (CLT), we simulate 1000 differences in donation rates between treatment and control groups, for different sample sizes (50, 200, 500, 1000). For each iteration, we draw n samples from each group and compute the average difference.\nThis allows us to observe how the sampling distribution of the difference-in-means becomes more concentrated and bell-shaped as sample size increases.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set seed\nnp.random.seed(42)\n\n# Parameters\np_control = 0.018\np_treatment = 0.022\niterations = 1000\nsample_sizes = [50, 200, 500, 1000]\n\n# Prepare plot\nfig, axs = plt.subplots(2, 2, figsize=(8, 6))\naxs = axs.flatten()\n\n# Loop over sample sizes\nfor i, n in enumerate(sample_sizes):\n    diffs = []\n    for _ in range(iterations):\n        c = np.random.binomial(1, p_control, size=n)\n        t = np.random.binomial(1, p_treatment, size=n)\n        diffs.append(t.mean() - c.mean())\n    axs[i].hist(diffs, bins=30, color='#1f77b4', edgecolor='white')\n    axs[i].axvline(0, color='black', linestyle='--', label='Zero')\n    axs[i].axvline(0.004, color='red', linestyle='--', label='True Diff')\n    axs[i].set_title(f\"Sample Size = {n}\")\n    axs[i].set_xlabel(\"Difference in Means\")\n    axs[i].set_ylabel(\"Frequency\")\n\nplt.suptitle(\"Demonstrating Central Limit Theorem: Sampling Distributions\", fontsize=14)\nplt.tight_layout()\nplt.show()\n\n\n\n\nSampling Distributions of Mean Differences at Varying Sample Sizes\n\n\n\n\nThese four histograms show the sampling distribution of the difference in mean donation rates between treatment and control, simulated 1000 times at various sample sizes.\n\nAt n = 50, the distribution is jagged, wide, and often centered around zero. This means that with small samples, there’s a lot of noise, and treatment effects may not be detectable.\nAt n = 200, the distribution becomes more bell-shaped. The peak is slightly to the right of 0 — near the true mean difference (0.004) — but zero is still near the center, implying inconclusive results are still likely.\nAt n = 500, the distribution is much tighter. Zero is closer to the tail, and the bulk of the mass is near 0.004, which means that we’re starting to reliably detect the treatment effect.\nAt n = 1000, the distribution is tightly centered near 0.004 and zero is clearly in the tail. With this sample size, we would almost always reject the null of no effect.\n\nThis simulation is a clear illustration of the Central Limit Theorem: As sample size increases, the distribution of the sample mean difference becomes more normal, less variable and converges to the true treatment effect (0.004).\nIt also explains why large sample sizes matter in A/B testing and field experiments — we need them to reliably detect small effects and to be confident in our conclusions."
  },
  {
    "objectID": "blog/project2/index.html",
    "href": "blog/project2/index.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\nimport pandas as pd\n\ndf = pd.read_csv(\"blueprinty.csv\")\ndf.head()\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1500 entries, 0 to 1499\nData columns (total 4 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   patents     1500 non-null   int64  \n 1   region      1500 non-null   object \n 2   age         1500 non-null   float64\n 3   iscustomer  1500 non-null   int64  \ndtypes: float64(1), int64(2), object(1)\nmemory usage: 47.0+ KB\n\n\nThe dataset contains 1,500 observations across 4 columns:\n\npatents: A non-negative integer count of awarded patents over 5 years — this will be our response variable.\nregion: A categorical variable indicating where the firm is located.\nage: A numeric variable indicating the number of years since the firm was incorporated.\niscustomer: A binary indicator (0/1) for whether the firm uses Blueprinty’s software.\n\nWe now examine whether firms that use Blueprinty’s software tend to receive more patents. We do this by comparing the distribution of patents awarded for Blueprinty customers vs. non-customers and the mean number of patents awarded in each group.\n\nimport matplotlib.pyplot as plt\n\n# Split data by customer status\ncust = df[df['iscustomer'] == 1]['patents']\nnoncust = df[df['iscustomer'] == 0]['patents']\n\n# Calculate means\nmean_cust = cust.mean()\nmean_noncust = noncust.mean()\n\n# Plot histograms\nplt.figure(figsize=(8, 5))\n\nplt.hist([noncust, cust], bins=range(0, 17), label=[f\"Non-Customers (mean={mean_noncust:.2f})\", f\"Customers (mean={mean_cust:.2f})\"], \n         color=[\"#ff7f0e\", \"#1f77b4\"], edgecolor='white', rwidth=0.9)\n\nplt.axvline(mean_noncust, color='#ff7f0e', linestyle='--')\nplt.axvline(mean_cust, color='#1f77b4', linestyle='--')\n\nplt.xlabel(\"Number of Patents Awarded\")\nplt.ylabel(\"Number of Firms\")\nplt.title(\"Patent Counts by Blueprinty Customer Status\")\nplt.legend()\nplt.grid(axis='y', linestyle='--', alpha=0.5)\nplt.tight_layout()\nplt.show()\n\n\n\n\nHistogram of Patents Awarded: Blueprinty Customers vs Non-Customers\n\n\n\n\nThe histogram compares the distribution of patents awarded between Blueprinty customers (blue) and non-customers (orange). We observe that customers tend to have higher average patent counts — about 4.13 compared to 3.47 for non-customers — as indicated by the vertical dashed lines.\nWhile both distributions overlap considerably, Blueprinty customers are less likely to have zero or one patent and more likely to appear in the higher end of the distribution (e.g., 6 or more patents). This suggests that Blueprinty users, on average, are somewhat more successful in obtaining patents. However, the overlap also highlights that many non-customers perform just as well as customers, underscoring the need for a more rigorous statistical model to isolate the effect of software use from other factors.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\nimport seaborn as sns\n\nfig, axs = plt.subplots(2, 1, figsize=(7, 9))\n\n# Region: stacked bar of customer status\nregion_counts = pd.crosstab(df['region'], df['iscustomer'], normalize='index') * 100\nregion_counts.plot(kind='bar', stacked=True, ax=axs[0], color=['#ff7f0e', '#1f77b4'])\naxs[0].set_title(\"Customer Status by Region\")\naxs[0].set_ylabel(\"Percentage of Firms\")\naxs[0].legend(title=\"Is Customer\", labels=[\"No\", \"Yes\"], loc='lower left')\naxs[0].tick_params(axis='x', rotation=45)\n\n# Age distribution by customer status\nsns.histplot(data=df, x='age', hue='iscustomer', bins=30, ax=axs[1], element='step', stat='density', common_norm=False)\naxs[1].set_title(\"Age Distribution by Customer Status\")\naxs[1].set_xlabel(\"Years Since Incorporation\")\naxs[1].legend(title=\"Is Customer\", labels=[\"No\", \"Yes\"])\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nComparison of Region and Age by Customer Status\n\n\n\n\nWe observe some regional variation in customer status: certain regions have higher proportions of Blueprinty users. For example, customers may be overrepresented in regions with more technology-focused industries or firms with higher patenting activity.\nRegarding age, the distribution of firm age is broadly similar across customers and non-customers, but there may be a slight tendency for Blueprinty customers to be younger on average. This could reflect greater technology adoption among newer firms or those more actively investing in patent tools.\nThese differences suggest that region and age may confound the relationship between Blueprinty use and patent output, and thus should be included as controls in the Poisson regression model.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nWe assume that the number of patents awarded to each firm over 5 years, denoted \\(Y_i\\), follows a Poisson distribution:\n\\[\nY_i \\sim \\text{Poisson}(\\lambda_i)\n\\]\nThe probability mass function for a Poisson distribution is:\n\\[\nf(Y_i \\mid \\lambda_i) = \\frac{e^{-\\lambda_i} \\lambda_i^{Y_i}}{Y_i!}\n\\]\nAssuming independence across firms, the likelihood function for a sample of \\(n\\) firms is:\n\\[\nL(\\lambda) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda_i} \\lambda_i^{Y_i}}{Y_i!}\n\\]\nTaking logs gives the log-likelihood function:\n\\[\n\\log L(\\lambda) = \\sum_{i=1}^{n} \\left[ -\\lambda_i + Y_i \\log \\lambda_i - \\log(Y_i!) \\right]\n\\]\nIn a regression context, we typically model \\(\\lambda_i\\) as a function of covariates:\n\\[\n\\lambda_i = \\exp(X_i \\beta)\n\\]\nThis formulation ensures that \\(\\lambda_i &gt; 0\\) for all \\(i\\), and allows us to estimate \\(\\beta\\) using Maximum Likelihood.\nTo estimate the Poisson model via Maximum Likelihood, we first define the log-likelihood function. This function takes the observed patent counts and a vector of Poisson rate parameters \\(\\lambda\\), and returns the total log-likelihood across all firms.\n\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_log_likelihood(lam, y, dtype=float):\n    \"\"\"\n    Computes the log-likelihood of observing y given lambda for a Poisson model.\n\n    Parameters:\n    - lam: array-like of predicted Poisson means (lambda_i)\n    - y: array-like of observed counts (y_i)\n\n    Returns:\n    - log-likelihood (float)\n    \"\"\"\n\n    lam = np.array(lam)\n    y = np.array(y)\n\n    # Avoid log(0)\n    lam = np.clip(lam, 1e-10, None)\n\n    # Apply Poisson log-likelihood formula\n    loglik = np.sum(-lam + y * np.log(lam) - gammaln(y + 1))\n    return loglik\n\nTo visualize how the Poisson log-likelihood changes with different values of \\(\\lambda\\), we compute the log-likelihood over a range of candidate \\(\\lambda\\) values, holding \\(\\lambda\\) constant across all observations. This approach assumes a simplified model where all firms are expected to receive the same average number of patents. We then plot the resulting log-likelihood curve.\nThe peak of the curve represents the Maximum Likelihood Estimate (MLE) of \\(\\lambda\\), which is the value that makes the observed data most probable under the Poisson model.\n\n# Use observed Y (patents) as data\nY = df['patents'].values\n\n# Create a range of lambda values to try\nlambda_vals = np.linspace(0.1, 10, 1000) \n\n# Compute log-likelihood for each λ\nloglik_vals = [poisson_log_likelihood(lam=np.full_like(Y, l, dtype=float), y=Y) for l in lambda_vals]\n\n# Find MLE λ and its log-likelihood\nmax_idx = np.argmax(loglik_vals)\nmle_lambda = lambda_vals[max_idx]\nmle_loglik = loglik_vals[max_idx]\n\n# Plot\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_vals, loglik_vals, color='navy')\nplt.axvline(mle_lambda, color='red', linestyle='--', label=\"MLE λ\")\nplt.text(mle_lambda + 0.1, mle_loglik - 10000, f\"MLE λ ≈ {mle_lambda:.3f}\", color='red')\n\nplt.xlabel(\"Lambda (λ)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.title(\"Poisson Log-Likelihood across Values of λ\")\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.tight_layout()\nplt.show()\n\n\n\n\nLog-Likelihood of Poisson Model for Different Lambda Values\n\n\n\n\nWe evaluate the log-likelihood of the Poisson model across a range of values for the rate parameter \\(\\lambda\\), holding it constant across all observations. The peak of the curve represents the maximum likelihood estimate (MLE) of \\(\\lambda\\).\nAs shown in the plot, the log-likelihood reaches its maximum when \\(\\lambda \\approx\\) 3.687, which matches the sample mean of the observed patent counts. This is consistent with the known fact that the MLE of λ in a Poisson model with constant λ is simply the sample mean.\nWe assume that each observation \\(Y_i\\) is drawn independently from a Poisson distribution:\n\\[\nY_i \\sim \\text{Poisson}(\\lambda)\n\\]\nThe likelihood function is:\n\\[\nL(\\lambda) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nTaking logs gives the log-likelihood:\n\\[\n\\log L(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log \\lambda - \\log(Y_i!) \\right)\n\\]\nNow take the first derivative with respect to \\(\\lambda\\):\n\\[\n\\frac{d}{d\\lambda} \\log L(\\lambda) = \\sum_{i=1}^{n} \\left( -1 + \\frac{Y_i}{\\lambda} \\right)\n= -n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i\n\\]\nSet the derivative equal to zero:\n\\[\n-n + \\frac{1}{\\lambda} \\sum Y_i = 0\n\\quad \\Rightarrow \\quad \\lambda = \\frac{1}{n} \\sum Y_i = \\bar{Y}\n\\]\nSo the MLE for \\(\\lambda\\) is simply the sample mean:\n\\[\n\\hat{\\lambda}_{\\text{MLE}} = \\bar{Y}\n\\]\nThis result “feels right” because the mean of a Poisson distribution is, by definition, \\(\\lambda\\). When estimating a constant \\(\\lambda\\) from independent Poisson observations, the natural estimator is just the average of the data.\nWe now use numerical optimization to find the MLE for \\(\\lambda\\) by maximizing the log-likelihood function. Since scipy.optimize.minimize() minimizes functions, we minimize the negative log-likelihood instead.\n\nfrom scipy.optimize import minimize\nimport numpy as np\n\n# Observed Y\nY = df['patents'].values\n\n# Define a wrapper: negative log-likelihood for scalar lambda\ndef neg_loglik_scalar(l):\n    lam_vec = np.full(Y.shape, l[0], dtype=float)\n    return -poisson_log_likelihood(lam_vec, Y)\n\n# Initial guess (e.g., lambda = 1)\ninitial_guess = [1.0]\n\n# Optimize\nresult = minimize(neg_loglik_scalar, initial_guess, bounds=[(1e-5, None)])\n\n# Extract MLE\nmle_lambda = result.x[0]\nprint(f\"Estimated MLE λ from optimization: {result.x[0]:.4f}\")\n\nEstimated MLE λ from optimization: 3.6847\n\n\nThe function neg_loglik_scalar() returns the negative Poisson log-likelihood given a constant \\(\\lambda\\) across all observations. After optimization, we find that the MLE value of \\(\\lambda\\) closely matches the sample mean of patents, confirming both our theoretical and visual findings.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nThe following function computes the log-likelihood for this model. It takes as input:\n\n\\(\\beta\\): a vector of regression coefficients\n\\(X\\): a covariate matrix (e.g., age, age squared, region dummies, customer status)\n\\(y\\): observed patent counts\n\nWe return the negative log-likelihood to allow optimization with scipy.optimize.minimize(), which minimizes by default.\n\ndef poisson_regression_log_likelihood(beta, y, X):\n    \"\"\"\n    Log-likelihood function for Poisson regression.\n\n    Parameters:\n    - beta: array-like, shape (p,)\n    - y: array-like, shape (n,)\n    - X: array-like, shape (n, p)\n\n    Returns:\n    - Negative log-likelihood (float)\n    \"\"\"\n    beta = np.array(beta)\n    X = np.array(X)\n    y = np.array(y)\n\n    # Compute lambda_i = exp(X @ beta)\n    linpred = X @ beta\n    lam = np.exp(linpred)\n\n    lam = np.clip(lam, 1e-10, 1e6)\n\n    # Log-likelihood\n    loglik = np.sum(-lam + y * np.log(lam) - gammaln(y + 1))\n    return -loglik\n\nThis function allows us to evaluate how well any given coefficient vector \\(\\beta\\) explains the observed patent counts. In the next step, we will use this function to find the MLE of \\(\\beta\\) via numerical optimization.\nWe now estimate the Poisson regression model using our custom log-likelihood function. The covariate matrix \\(X\\) includes:\n\nA constant intercept term\nFirm age and age squared\nRegion dummy variables (leaving one region as the reference group)\nA binary variable indicating whether the firm is a customer of Blueprinty\n\nAfter estimating the model via maximum likelihood, we compute standard errors using the inverse of the Hessian matrix.\n\n# Prepare data\ndf['age_sq'] = df['age'] ** 2\n\n# Create dummy variables for region (drop one to avoid multicollinearity)\nregion_dummies = pd.get_dummies(df['region'], drop_first=True)\n\n# Design matrix X\nX = pd.concat([\n    pd.Series(1, index=df.index, name='intercept'),  # constant term\n    df[['age', 'age_sq']],\n    region_dummies,\n    df[['iscustomer']]\n], axis=1)\n\n# Outcome variable y\ny = df['patents'].values\nX_matrix = X.values\ninitial_beta = np.zeros(X.shape[1])\n\n# Optimize\nresult = minimize(\n    poisson_regression_log_likelihood,\n    x0=initial_beta,\n    args=(y, X_matrix),\n    method='BFGS'\n)\n\n# Estimated coefficients\nbeta_hat = result.x\n\n# Variance-covariance matrix (inverse of Hessian)\nvcov = result.hess_inv\n\n# Standard errors\nse = np.sqrt(np.diag(vcov))\n\n# Create a summary table\nsummary_table = pd.DataFrame({\n    'Coefficient': beta_hat,\n    'Std. Error': se\n}, index=X.columns)\n\nsummary_table\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\n\n\n\n\nintercept\n-0.509981\n0.211822\n\n\nage\n0.148705\n0.015731\n\n\nage_sq\n-0.002972\n0.000287\n\n\nNortheast\n0.029155\n0.044500\n\n\nNorthwest\n-0.017575\n0.063477\n\n\nSouth\n0.056567\n0.057164\n\n\nSouthwest\n0.050564\n0.051574\n\n\niscustomer\n0.207607\n0.035479\n\n\n\n\n\n\n\nTo verify our custom MLE implementation, we compare our results with the output of Python’s built-in statsmodels.GLM() function. This function fits a Poisson regression model using iteratively reweighted least squares (IRLS), a standard and numerically stable algorithm.\nWe use the same design matrix X_matrix and response variable y, specifying a Poisson family with a log link function.\n\nimport statsmodels.api as sm\n\n# Fit Poisson GLM\nmodel = sm.GLM(y, X_matrix, family=sm.families.Poisson())\nresult_glm = model.fit()\n\n# Output summary\nprint(result_glm.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3258.1\nDate:                Sat, 03 May 2025   Deviance:                       2143.3\nTime:                        12:03:48   Pearson chi2:                 2.07e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1360\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -0.5089      0.183     -2.778      0.005      -0.868      -0.150\nx1             0.1486      0.014     10.716      0.000       0.121       0.176\nx2            -0.0030      0.000    -11.513      0.000      -0.003      -0.002\nx3             0.0292      0.044      0.669      0.504      -0.056       0.115\nx4            -0.0176      0.054     -0.327      0.744      -0.123       0.088\nx5             0.0566      0.053      1.074      0.283      -0.047       0.160\nx6             0.0506      0.047      1.072      0.284      -0.042       0.143\nx7             0.2076      0.031      6.719      0.000       0.147       0.268\n==============================================================================\n\n\nTo make the comparison even more transparent, we compile the coefficient estimates and standard errors from both methods into a single table.\n\n# Combine manual MLE and GLM results into one table\ncomparison_table = pd.DataFrame({\n    'Manual MLE Coef': beta_hat,\n    'GLM Coef': result_glm.params,\n    'Manual SE': se,\n    'GLM SE': result_glm.bse\n}, index=X.columns)\n\n# Round for clarity\ncomparison_table = comparison_table.round(4)\ncomparison_table\n\n\n\n\n\n\n\n\nManual MLE Coef\nGLM Coef\nManual SE\nGLM SE\n\n\n\n\nintercept\n-0.5100\n-0.5089\n0.2118\n0.1832\n\n\nage\n0.1487\n0.1486\n0.0157\n0.0139\n\n\nage_sq\n-0.0030\n-0.0030\n0.0003\n0.0003\n\n\nNortheast\n0.0292\n0.0292\n0.0445\n0.0436\n\n\nNorthwest\n-0.0176\n-0.0176\n0.0635\n0.0538\n\n\nSouth\n0.0566\n0.0566\n0.0572\n0.0527\n\n\nSouthwest\n0.0506\n0.0506\n0.0516\n0.0472\n\n\niscustomer\n0.2076\n0.2076\n0.0355\n0.0309\n\n\n\n\n\n\n\nThe table above compares the coefficient estimates and standard errors obtained from our custom Poisson MLE implementation with those produced by the built-in statsmodels.GLM() function. We can find that the estimated coefficients from both methods match to at least four decimal places, confirming that our manual likelihood function and optimization routine were implemented correctly, and the standard errors are also nearly identical, with only minor numerical differences attributable to the optimization algorithm or finite difference approximations.\nThis agreement provides strong validation that our custom MLE estimation procedure correctly reproduces the results of a professionally implemented Poisson regression. It also reinforces the interpretation of the model: for example, the coefficient on iscustomer is approximately 0.208, suggesting that using Blueprinty’s software is associated with a higher expected patent count, holding other variables constant.\nSince Poisson regression coefficients are on the log scale, they are not directly interpretable as raw changes in count outcomes. To better understand the practical effect of using Blueprinty’s software, we conduct a simple simulation:\nWe create two versions of the dataset:\n\nX_0: identical to the original X, but assumes no firm is a customer (iscustomer = 0)\nX_1: identical to X, but assumes all firms are customers (iscustomer = 1)\n\nWe then use the estimated \\(\\hat\\beta\\) vector to predict expected patent counts under both scenarios. The average difference between these two sets of predictions gives an estimate of the causal effect of Blueprinty’s software on patent output.\n\n# Create X_0 and X_1\nX_0 = X.copy()\nX_0['iscustomer'] = 0\n\nX_1 = X.copy()\nX_1['iscustomer'] = 1\n\n# Predicted lambdas\ny_pred_0 = np.exp(X_0 @ beta_hat)\ny_pred_1 = np.exp(X_1 @ beta_hat)\n\n# Difference\ndiff = y_pred_1 - y_pred_0\naverage_diff = diff.mean()\n\nprint(f\"Average predicted increase in patents from using Blueprinty: {average_diff:.3f}\")\n\nAverage predicted increase in patents from using Blueprinty: 0.793\n\n\nThe result indicates that, on average, using Blueprinty is associated with an increase of approximately 0.793 patents per firm over a five-year period, holding all other firm characteristics constant.\nWhile this number may appear modest at first glance, it is both statistically significant and practically meaningful in the context of patent production, where the average firm receives fewer than 4 patents over 5 years. This suggests that Blueprinty’s software is a valuable tool for firms looking to increase their patenting success — a claim that the company can reasonably include in its marketing efforts."
  },
  {
    "objectID": "blog/project2/index.html#blueprinty-case-study",
    "href": "blog/project2/index.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\nimport pandas as pd\n\ndf = pd.read_csv(\"blueprinty.csv\")\ndf.head()\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1500 entries, 0 to 1499\nData columns (total 4 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   patents     1500 non-null   int64  \n 1   region      1500 non-null   object \n 2   age         1500 non-null   float64\n 3   iscustomer  1500 non-null   int64  \ndtypes: float64(1), int64(2), object(1)\nmemory usage: 47.0+ KB\n\n\nThe dataset contains 1,500 observations across 4 columns:\n\npatents: A non-negative integer count of awarded patents over 5 years — this will be our response variable.\nregion: A categorical variable indicating where the firm is located.\nage: A numeric variable indicating the number of years since the firm was incorporated.\niscustomer: A binary indicator (0/1) for whether the firm uses Blueprinty’s software.\n\nWe now examine whether firms that use Blueprinty’s software tend to receive more patents. We do this by comparing the distribution of patents awarded for Blueprinty customers vs. non-customers and the mean number of patents awarded in each group.\n\nimport matplotlib.pyplot as plt\n\n# Split data by customer status\ncust = df[df['iscustomer'] == 1]['patents']\nnoncust = df[df['iscustomer'] == 0]['patents']\n\n# Calculate means\nmean_cust = cust.mean()\nmean_noncust = noncust.mean()\n\n# Plot histograms\nplt.figure(figsize=(8, 5))\n\nplt.hist([noncust, cust], bins=range(0, 17), label=[f\"Non-Customers (mean={mean_noncust:.2f})\", f\"Customers (mean={mean_cust:.2f})\"], \n         color=[\"#ff7f0e\", \"#1f77b4\"], edgecolor='white', rwidth=0.9)\n\nplt.axvline(mean_noncust, color='#ff7f0e', linestyle='--')\nplt.axvline(mean_cust, color='#1f77b4', linestyle='--')\n\nplt.xlabel(\"Number of Patents Awarded\")\nplt.ylabel(\"Number of Firms\")\nplt.title(\"Patent Counts by Blueprinty Customer Status\")\nplt.legend()\nplt.grid(axis='y', linestyle='--', alpha=0.5)\nplt.tight_layout()\nplt.show()\n\n\n\n\nHistogram of Patents Awarded: Blueprinty Customers vs Non-Customers\n\n\n\n\nThe histogram compares the distribution of patents awarded between Blueprinty customers (blue) and non-customers (orange). We observe that customers tend to have higher average patent counts — about 4.13 compared to 3.47 for non-customers — as indicated by the vertical dashed lines.\nWhile both distributions overlap considerably, Blueprinty customers are less likely to have zero or one patent and more likely to appear in the higher end of the distribution (e.g., 6 or more patents). This suggests that Blueprinty users, on average, are somewhat more successful in obtaining patents. However, the overlap also highlights that many non-customers perform just as well as customers, underscoring the need for a more rigorous statistical model to isolate the effect of software use from other factors.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\nimport seaborn as sns\n\nfig, axs = plt.subplots(2, 1, figsize=(7, 9))\n\n# Region: stacked bar of customer status\nregion_counts = pd.crosstab(df['region'], df['iscustomer'], normalize='index') * 100\nregion_counts.plot(kind='bar', stacked=True, ax=axs[0], color=['#ff7f0e', '#1f77b4'])\naxs[0].set_title(\"Customer Status by Region\")\naxs[0].set_ylabel(\"Percentage of Firms\")\naxs[0].legend(title=\"Is Customer\", labels=[\"No\", \"Yes\"], loc='lower left')\naxs[0].tick_params(axis='x', rotation=45)\n\n# Age distribution by customer status\nsns.histplot(data=df, x='age', hue='iscustomer', bins=30, ax=axs[1], element='step', stat='density', common_norm=False)\naxs[1].set_title(\"Age Distribution by Customer Status\")\naxs[1].set_xlabel(\"Years Since Incorporation\")\naxs[1].legend(title=\"Is Customer\", labels=[\"No\", \"Yes\"])\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nComparison of Region and Age by Customer Status\n\n\n\n\nWe observe some regional variation in customer status: certain regions have higher proportions of Blueprinty users. For example, customers may be overrepresented in regions with more technology-focused industries or firms with higher patenting activity.\nRegarding age, the distribution of firm age is broadly similar across customers and non-customers, but there may be a slight tendency for Blueprinty customers to be younger on average. This could reflect greater technology adoption among newer firms or those more actively investing in patent tools.\nThese differences suggest that region and age may confound the relationship between Blueprinty use and patent output, and thus should be included as controls in the Poisson regression model.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nWe assume that the number of patents awarded to each firm over 5 years, denoted \\(Y_i\\), follows a Poisson distribution:\n\\[\nY_i \\sim \\text{Poisson}(\\lambda_i)\n\\]\nThe probability mass function for a Poisson distribution is:\n\\[\nf(Y_i \\mid \\lambda_i) = \\frac{e^{-\\lambda_i} \\lambda_i^{Y_i}}{Y_i!}\n\\]\nAssuming independence across firms, the likelihood function for a sample of \\(n\\) firms is:\n\\[\nL(\\lambda) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda_i} \\lambda_i^{Y_i}}{Y_i!}\n\\]\nTaking logs gives the log-likelihood function:\n\\[\n\\log L(\\lambda) = \\sum_{i=1}^{n} \\left[ -\\lambda_i + Y_i \\log \\lambda_i - \\log(Y_i!) \\right]\n\\]\nIn a regression context, we typically model \\(\\lambda_i\\) as a function of covariates:\n\\[\n\\lambda_i = \\exp(X_i \\beta)\n\\]\nThis formulation ensures that \\(\\lambda_i &gt; 0\\) for all \\(i\\), and allows us to estimate \\(\\beta\\) using Maximum Likelihood.\nTo estimate the Poisson model via Maximum Likelihood, we first define the log-likelihood function. This function takes the observed patent counts and a vector of Poisson rate parameters \\(\\lambda\\), and returns the total log-likelihood across all firms.\n\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_log_likelihood(lam, y, dtype=float):\n    \"\"\"\n    Computes the log-likelihood of observing y given lambda for a Poisson model.\n\n    Parameters:\n    - lam: array-like of predicted Poisson means (lambda_i)\n    - y: array-like of observed counts (y_i)\n\n    Returns:\n    - log-likelihood (float)\n    \"\"\"\n\n    lam = np.array(lam)\n    y = np.array(y)\n\n    # Avoid log(0)\n    lam = np.clip(lam, 1e-10, None)\n\n    # Apply Poisson log-likelihood formula\n    loglik = np.sum(-lam + y * np.log(lam) - gammaln(y + 1))\n    return loglik\n\nTo visualize how the Poisson log-likelihood changes with different values of \\(\\lambda\\), we compute the log-likelihood over a range of candidate \\(\\lambda\\) values, holding \\(\\lambda\\) constant across all observations. This approach assumes a simplified model where all firms are expected to receive the same average number of patents. We then plot the resulting log-likelihood curve.\nThe peak of the curve represents the Maximum Likelihood Estimate (MLE) of \\(\\lambda\\), which is the value that makes the observed data most probable under the Poisson model.\n\n# Use observed Y (patents) as data\nY = df['patents'].values\n\n# Create a range of lambda values to try\nlambda_vals = np.linspace(0.1, 10, 1000) \n\n# Compute log-likelihood for each λ\nloglik_vals = [poisson_log_likelihood(lam=np.full_like(Y, l, dtype=float), y=Y) for l in lambda_vals]\n\n# Find MLE λ and its log-likelihood\nmax_idx = np.argmax(loglik_vals)\nmle_lambda = lambda_vals[max_idx]\nmle_loglik = loglik_vals[max_idx]\n\n# Plot\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_vals, loglik_vals, color='navy')\nplt.axvline(mle_lambda, color='red', linestyle='--', label=\"MLE λ\")\nplt.text(mle_lambda + 0.1, mle_loglik - 10000, f\"MLE λ ≈ {mle_lambda:.3f}\", color='red')\n\nplt.xlabel(\"Lambda (λ)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.title(\"Poisson Log-Likelihood across Values of λ\")\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.tight_layout()\nplt.show()\n\n\n\n\nLog-Likelihood of Poisson Model for Different Lambda Values\n\n\n\n\nWe evaluate the log-likelihood of the Poisson model across a range of values for the rate parameter \\(\\lambda\\), holding it constant across all observations. The peak of the curve represents the maximum likelihood estimate (MLE) of \\(\\lambda\\).\nAs shown in the plot, the log-likelihood reaches its maximum when \\(\\lambda \\approx\\) 3.687, which matches the sample mean of the observed patent counts. This is consistent with the known fact that the MLE of λ in a Poisson model with constant λ is simply the sample mean.\nWe assume that each observation \\(Y_i\\) is drawn independently from a Poisson distribution:\n\\[\nY_i \\sim \\text{Poisson}(\\lambda)\n\\]\nThe likelihood function is:\n\\[\nL(\\lambda) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nTaking logs gives the log-likelihood:\n\\[\n\\log L(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log \\lambda - \\log(Y_i!) \\right)\n\\]\nNow take the first derivative with respect to \\(\\lambda\\):\n\\[\n\\frac{d}{d\\lambda} \\log L(\\lambda) = \\sum_{i=1}^{n} \\left( -1 + \\frac{Y_i}{\\lambda} \\right)\n= -n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i\n\\]\nSet the derivative equal to zero:\n\\[\n-n + \\frac{1}{\\lambda} \\sum Y_i = 0\n\\quad \\Rightarrow \\quad \\lambda = \\frac{1}{n} \\sum Y_i = \\bar{Y}\n\\]\nSo the MLE for \\(\\lambda\\) is simply the sample mean:\n\\[\n\\hat{\\lambda}_{\\text{MLE}} = \\bar{Y}\n\\]\nThis result “feels right” because the mean of a Poisson distribution is, by definition, \\(\\lambda\\). When estimating a constant \\(\\lambda\\) from independent Poisson observations, the natural estimator is just the average of the data.\nWe now use numerical optimization to find the MLE for \\(\\lambda\\) by maximizing the log-likelihood function. Since scipy.optimize.minimize() minimizes functions, we minimize the negative log-likelihood instead.\n\nfrom scipy.optimize import minimize\nimport numpy as np\n\n# Observed Y\nY = df['patents'].values\n\n# Define a wrapper: negative log-likelihood for scalar lambda\ndef neg_loglik_scalar(l):\n    lam_vec = np.full(Y.shape, l[0], dtype=float)\n    return -poisson_log_likelihood(lam_vec, Y)\n\n# Initial guess (e.g., lambda = 1)\ninitial_guess = [1.0]\n\n# Optimize\nresult = minimize(neg_loglik_scalar, initial_guess, bounds=[(1e-5, None)])\n\n# Extract MLE\nmle_lambda = result.x[0]\nprint(f\"Estimated MLE λ from optimization: {result.x[0]:.4f}\")\n\nEstimated MLE λ from optimization: 3.6847\n\n\nThe function neg_loglik_scalar() returns the negative Poisson log-likelihood given a constant \\(\\lambda\\) across all observations. After optimization, we find that the MLE value of \\(\\lambda\\) closely matches the sample mean of patents, confirming both our theoretical and visual findings.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nThe following function computes the log-likelihood for this model. It takes as input:\n\n\\(\\beta\\): a vector of regression coefficients\n\\(X\\): a covariate matrix (e.g., age, age squared, region dummies, customer status)\n\\(y\\): observed patent counts\n\nWe return the negative log-likelihood to allow optimization with scipy.optimize.minimize(), which minimizes by default.\n\ndef poisson_regression_log_likelihood(beta, y, X):\n    \"\"\"\n    Log-likelihood function for Poisson regression.\n\n    Parameters:\n    - beta: array-like, shape (p,)\n    - y: array-like, shape (n,)\n    - X: array-like, shape (n, p)\n\n    Returns:\n    - Negative log-likelihood (float)\n    \"\"\"\n    beta = np.array(beta)\n    X = np.array(X)\n    y = np.array(y)\n\n    # Compute lambda_i = exp(X @ beta)\n    linpred = X @ beta\n    lam = np.exp(linpred)\n\n    lam = np.clip(lam, 1e-10, 1e6)\n\n    # Log-likelihood\n    loglik = np.sum(-lam + y * np.log(lam) - gammaln(y + 1))\n    return -loglik\n\nThis function allows us to evaluate how well any given coefficient vector \\(\\beta\\) explains the observed patent counts. In the next step, we will use this function to find the MLE of \\(\\beta\\) via numerical optimization.\nWe now estimate the Poisson regression model using our custom log-likelihood function. The covariate matrix \\(X\\) includes:\n\nA constant intercept term\nFirm age and age squared\nRegion dummy variables (leaving one region as the reference group)\nA binary variable indicating whether the firm is a customer of Blueprinty\n\nAfter estimating the model via maximum likelihood, we compute standard errors using the inverse of the Hessian matrix.\n\n# Prepare data\ndf['age_sq'] = df['age'] ** 2\n\n# Create dummy variables for region (drop one to avoid multicollinearity)\nregion_dummies = pd.get_dummies(df['region'], drop_first=True)\n\n# Design matrix X\nX = pd.concat([\n    pd.Series(1, index=df.index, name='intercept'),  # constant term\n    df[['age', 'age_sq']],\n    region_dummies,\n    df[['iscustomer']]\n], axis=1)\n\n# Outcome variable y\ny = df['patents'].values\nX_matrix = X.values\ninitial_beta = np.zeros(X.shape[1])\n\n# Optimize\nresult = minimize(\n    poisson_regression_log_likelihood,\n    x0=initial_beta,\n    args=(y, X_matrix),\n    method='BFGS'\n)\n\n# Estimated coefficients\nbeta_hat = result.x\n\n# Variance-covariance matrix (inverse of Hessian)\nvcov = result.hess_inv\n\n# Standard errors\nse = np.sqrt(np.diag(vcov))\n\n# Create a summary table\nsummary_table = pd.DataFrame({\n    'Coefficient': beta_hat,\n    'Std. Error': se\n}, index=X.columns)\n\nsummary_table\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\n\n\n\n\nintercept\n-0.509981\n0.211822\n\n\nage\n0.148705\n0.015731\n\n\nage_sq\n-0.002972\n0.000287\n\n\nNortheast\n0.029155\n0.044500\n\n\nNorthwest\n-0.017575\n0.063477\n\n\nSouth\n0.056567\n0.057164\n\n\nSouthwest\n0.050564\n0.051574\n\n\niscustomer\n0.207607\n0.035479\n\n\n\n\n\n\n\nTo verify our custom MLE implementation, we compare our results with the output of Python’s built-in statsmodels.GLM() function. This function fits a Poisson regression model using iteratively reweighted least squares (IRLS), a standard and numerically stable algorithm.\nWe use the same design matrix X_matrix and response variable y, specifying a Poisson family with a log link function.\n\nimport statsmodels.api as sm\n\n# Fit Poisson GLM\nmodel = sm.GLM(y, X_matrix, family=sm.families.Poisson())\nresult_glm = model.fit()\n\n# Output summary\nprint(result_glm.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3258.1\nDate:                Sat, 03 May 2025   Deviance:                       2143.3\nTime:                        12:03:48   Pearson chi2:                 2.07e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1360\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -0.5089      0.183     -2.778      0.005      -0.868      -0.150\nx1             0.1486      0.014     10.716      0.000       0.121       0.176\nx2            -0.0030      0.000    -11.513      0.000      -0.003      -0.002\nx3             0.0292      0.044      0.669      0.504      -0.056       0.115\nx4            -0.0176      0.054     -0.327      0.744      -0.123       0.088\nx5             0.0566      0.053      1.074      0.283      -0.047       0.160\nx6             0.0506      0.047      1.072      0.284      -0.042       0.143\nx7             0.2076      0.031      6.719      0.000       0.147       0.268\n==============================================================================\n\n\nTo make the comparison even more transparent, we compile the coefficient estimates and standard errors from both methods into a single table.\n\n# Combine manual MLE and GLM results into one table\ncomparison_table = pd.DataFrame({\n    'Manual MLE Coef': beta_hat,\n    'GLM Coef': result_glm.params,\n    'Manual SE': se,\n    'GLM SE': result_glm.bse\n}, index=X.columns)\n\n# Round for clarity\ncomparison_table = comparison_table.round(4)\ncomparison_table\n\n\n\n\n\n\n\n\nManual MLE Coef\nGLM Coef\nManual SE\nGLM SE\n\n\n\n\nintercept\n-0.5100\n-0.5089\n0.2118\n0.1832\n\n\nage\n0.1487\n0.1486\n0.0157\n0.0139\n\n\nage_sq\n-0.0030\n-0.0030\n0.0003\n0.0003\n\n\nNortheast\n0.0292\n0.0292\n0.0445\n0.0436\n\n\nNorthwest\n-0.0176\n-0.0176\n0.0635\n0.0538\n\n\nSouth\n0.0566\n0.0566\n0.0572\n0.0527\n\n\nSouthwest\n0.0506\n0.0506\n0.0516\n0.0472\n\n\niscustomer\n0.2076\n0.2076\n0.0355\n0.0309\n\n\n\n\n\n\n\nThe table above compares the coefficient estimates and standard errors obtained from our custom Poisson MLE implementation with those produced by the built-in statsmodels.GLM() function. We can find that the estimated coefficients from both methods match to at least four decimal places, confirming that our manual likelihood function and optimization routine were implemented correctly, and the standard errors are also nearly identical, with only minor numerical differences attributable to the optimization algorithm or finite difference approximations.\nThis agreement provides strong validation that our custom MLE estimation procedure correctly reproduces the results of a professionally implemented Poisson regression. It also reinforces the interpretation of the model: for example, the coefficient on iscustomer is approximately 0.208, suggesting that using Blueprinty’s software is associated with a higher expected patent count, holding other variables constant.\nSince Poisson regression coefficients are on the log scale, they are not directly interpretable as raw changes in count outcomes. To better understand the practical effect of using Blueprinty’s software, we conduct a simple simulation:\nWe create two versions of the dataset:\n\nX_0: identical to the original X, but assumes no firm is a customer (iscustomer = 0)\nX_1: identical to X, but assumes all firms are customers (iscustomer = 1)\n\nWe then use the estimated \\(\\hat\\beta\\) vector to predict expected patent counts under both scenarios. The average difference between these two sets of predictions gives an estimate of the causal effect of Blueprinty’s software on patent output.\n\n# Create X_0 and X_1\nX_0 = X.copy()\nX_0['iscustomer'] = 0\n\nX_1 = X.copy()\nX_1['iscustomer'] = 1\n\n# Predicted lambdas\ny_pred_0 = np.exp(X_0 @ beta_hat)\ny_pred_1 = np.exp(X_1 @ beta_hat)\n\n# Difference\ndiff = y_pred_1 - y_pred_0\naverage_diff = diff.mean()\n\nprint(f\"Average predicted increase in patents from using Blueprinty: {average_diff:.3f}\")\n\nAverage predicted increase in patents from using Blueprinty: 0.793\n\n\nThe result indicates that, on average, using Blueprinty is associated with an increase of approximately 0.793 patents per firm over a five-year period, holding all other firm characteristics constant.\nWhile this number may appear modest at first glance, it is both statistically significant and practically meaningful in the context of patent production, where the average firm receives fewer than 4 patents over 5 years. This suggests that Blueprinty’s software is a valuable tool for firms looking to increase their patenting success — a claim that the company can reasonably include in its marketing efforts."
  },
  {
    "objectID": "blog/project2/index.html#airbnb-case-study",
    "href": "blog/project2/index.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\nWe begin by reading in the Airbnb dataset and performing some light data cleaning. Specifically, we drop any rows with missing values in key modeling variables, such as review scores, price, and room type. We also create dummy variables for categorical features like room_type and instant_bookable, and apply a log transformation to price to reduce skewness.\n\n# Load the dataset\ndf = pd.read_csv(\"airbnb.csv\")\n\n# Drop rows with missing values in modeling variables\ndf_clean = df.dropna(subset=[\n    'number_of_reviews', 'price', 'room_type', 'instant_bookable',\n    'review_scores_cleanliness', 'review_scores_location', 'review_scores_value',\n    'bathrooms', 'bedrooms'\n])\n\n# Create log-transformed price\ndf_clean['log_price'] = np.log1p(df_clean['price'])\n\n# Create dummy variables for room_type and instant_bookable\nroom_dummies = pd.get_dummies(df_clean['room_type'], drop_first=True)\nbookable_dummy = (df_clean['instant_bookable'] == \"t\").astype(int).rename(\"instant_bookable_dummy\")\n\n# Combine into one dataframe\ndf_model = pd.concat([\n    df_clean[['number_of_reviews', 'days', 'log_price',\n              'review_scores_cleanliness', 'review_scores_location',\n              'review_scores_value', 'bathrooms', 'bedrooms']],\n    room_dummies,\n    bookable_dummy\n], axis=1)\n\n# Preview the cleaned dataset\ndf_model.head()\n\n\n\n\n\n\n\n\nnumber_of_reviews\ndays\nlog_price\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\nbathrooms\nbedrooms\nPrivate room\nShared room\ninstant_bookable_dummy\n\n\n\n\n0\n150\n3130\n4.094345\n9.0\n9.0\n9.0\n1.0\n1.0\n1\n0\n0\n\n\n1\n20\n3127\n5.442418\n9.0\n10.0\n9.0\n1.0\n0.0\n0\n0\n0\n\n\n3\n116\n3038\n4.499810\n9.0\n9.0\n9.0\n1.0\n1.0\n0\n0\n0\n\n\n5\n60\n2981\n5.361292\n9.0\n9.0\n9.0\n1.0\n1.0\n0\n0\n0\n\n\n6\n60\n2981\n5.525453\n10.0\n9.0\n10.0\n1.0\n2.0\n0\n0\n0\n\n\n\n\n\n\n\nWe now have a cleaned dataset ready for modeling. The outcome variable number_of_reviews is a count variable, which we treat as a proxy for booking volume. The covariates include host characteristics (e.g. cleanliness ratings, room type, log price, etc.) that may help explain variation in review count across listings. We manually add a column of ones to include an intercept term.\n\n# Extract response variable\ny = df_model['number_of_reviews'].values\n\n# Define covariates: drop the outcome column\nX = df_model.drop(columns=['number_of_reviews'])\n\n# Add intercept column\nX.insert(0, 'intercept', 1)\n\n# Store design matrix and column names\nX_matrix = X.values\nX_columns = X.columns.tolist()\n\n# Preview shapes\nX.shape, y.shape\n\n((30160, 11), (30160,))\n\n\nWe now have a clean design matrix X_matrix with shape \\((30160, 11)\\) and a corresponding response vector y. In the next step, we will fit a Poisson regression model by maximum likelihood using these inputs.\nWe now fit a Poisson regression model to the Airbnb data using our custom log-likelihood function and scipy.optimize.minimize(). The outcome variable is the number of reviews (a proxy for bookings), and the predictors include room type, log price, review scores, and listing characteristics.\n\n# Define log-likelihood\ndef poisson_regression_log_likelihood(beta, y, X):\n    linpred = X @ beta\n    lam = np.exp(linpred)\n    lam = np.clip(lam, 1e-10, 1e6)  # ensure stability\n    loglik = np.sum(-lam + y * np.log(lam) - gammaln(y + 1))\n    return -loglik\n\n# Fit model\ninitial_beta = np.zeros(X_matrix.shape[1])\nresult_airbnb = minimize(poisson_regression_log_likelihood, x0=initial_beta,\n                         args=(y, X_matrix), method='BFGS')\n\n# Get estimates and standard errors\nbeta_hat = result_airbnb.x\nvcov = result_airbnb.hess_inv if isinstance(result_airbnb.hess_inv, np.ndarray) else result_airbnb.hess_inv.todense()\nse = np.sqrt(np.diag(vcov))\n\n# Create summary table\nsummary_airbnb = pd.DataFrame({\n    'Coefficient': beta_hat,\n    'Std. Error': se\n}, index=X_columns).round(4)\n\nsummary_airbnb\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\n\n\n\n\nintercept\n3.0133\n0.0221\n\n\ndays\n0.0001\n0.0000\n\n\nlog_price\n0.1311\n0.0035\n\n\nreview_scores_cleanliness\n0.1088\n0.0016\n\n\nreview_scores_location\n-0.0973\n0.0019\n\n\nreview_scores_value\n-0.0795\n0.0019\n\n\nbathrooms\n-0.1452\n0.0047\n\n\nbedrooms\n0.0465\n0.0025\n\n\nPrivate room\n0.0866\n0.0038\n\n\nShared room\n-0.1050\n0.0096\n\n\ninstant_bookable_dummy\n0.3520\n0.0032\n\n\n\n\n\n\n\nThe table summarizes the maximum likelihood estimates of the regression coefficients and their corresponding standard errors. These results quantify the association between listing characteristics and the expected number of reviews (a proxy for bookings) on Airbnb.\nThis table reports the estimated coefficients and standard errors from the Poisson regression model predicting the number of reviews, which we use as a proxy for booking volume. Since we are using a log-link function, each coefficient reflects the expected change in the log number of reviews for a one-unit change in the corresponding variable, holding all other variables constant.\nWe can interpret from the table that:\n\nlog_price (0.1311): Listings with higher prices tend to receive more reviews. This may reflect that higher-end listings are more visible or attract more attention.\nreview_scores_cleanliness (0.1088): Higher cleanliness scores are strongly associated with more reviews, indicating guest satisfaction contributes to engagement.\nreview_scores_location and review_scores_value: Surprisingly, both have negative coefficients. This could reflect expectations bias — listings in great locations may receive fewer marginal reviews due to less surprise or variance in experience.\nbathrooms (−0.1452): Listings with more bathrooms tend to receive fewer reviews, possibly because larger properties are used less frequently or are higher-end and niche.\nRoom Type:\n\nPrivate room (0.0866): Surprisingly positive, possibly due to affordability and appeal to solo travelers.\nShared room (−0.1050): Negative as expected — these may be less popular due to privacy concerns.\n\ninstant_bookable_dummy (0.3520): This is one of the strongest effects in the model. Since \\(\\exp(0.3520) \\approx 1.42\\), we interpret this as: listings that support instant booking receive approximately 42% more reviews, on average, than comparable listings that do not.\n\nThese findings align well with platform dynamics: clean, bookable, affordably priced listings tend to perform better in terms of guest engagement, as reflected in review counts.\nTo make the regression coefficients easier to interpret, we exponentiate them. This transforms each coefficient from a log scale into an incidence rate ratio (IRR). An IRR greater than 1 indicates a positive effect on the number of reviews, while a value less than 1 indicates a negative effect.\n\nX_columns = [\n    'intercept', 'days', 'log_price', 'review_scores_cleanliness',\n    'review_scores_location', 'review_scores_value', 'bathrooms',\n    'bedrooms', 'Private room', 'Shared room', 'instant_bookable_dummy'\n]\n\n# Compute exp(beta) and 95% confidence intervals\nexp_coef = np.exp(beta_hat)\nlower_ci = np.exp(beta_hat - 1.96 * se)\nupper_ci = np.exp(beta_hat + 1.96 * se)\n\n# Combine into table\nexp_table = pd.DataFrame({\n    'exp(Coefficient)': exp_coef,\n    'Lower 95% CI': lower_ci,\n    'Upper 95% CI': upper_ci\n}, index=X_columns).round(3)\n\nexp_table\n\n\n\n\n\n\n\n\nexp(Coefficient)\nLower 95% CI\nUpper 95% CI\n\n\n\n\nintercept\n20.355\n19.493\n21.255\n\n\ndays\n1.000\n1.000\n1.000\n\n\nlog_price\n1.140\n1.132\n1.148\n\n\nreview_scores_cleanliness\n1.115\n1.112\n1.118\n\n\nreview_scores_location\n0.907\n0.904\n0.911\n\n\nreview_scores_value\n0.924\n0.920\n0.927\n\n\nbathrooms\n0.865\n0.857\n0.873\n\n\nbedrooms\n1.048\n1.043\n1.053\n\n\nPrivate room\n1.090\n1.082\n1.098\n\n\nShared room\n0.900\n0.884\n0.917\n\n\ninstant_bookable_dummy\n1.422\n1.413\n1.431\n\n\n\n\n\n\n\nThe table reports \\(\\exp(\\beta)\\) for each variable, along with 95% confidence intervals. For example, the IRR for instant_bookable_dummy is 1.422, meaning that listings with instant booking enabled are predicted to receive 42.2% more reviews, on average, compared to those that are not. A variable like review_scores_location has an IRR of 0.907, suggesting a 9.3% decrease in expected review count per unit increase in location score — a counterintuitive result that may reflect deeper behavioral patterns.\nTo assess the robustness of our findings, we now estimate a simpler Poisson regression model using a reduced set of predictors. This model includes only core listing characteristics that are known before any guest stays: price, room type, exposure time (days), and booking mechanism (instant_bookable). We exclude review-based scores and amenities.\nThis helps isolate the effects of variables that are directly under host control at listing time.\n\n# Build reduced X matrix\nX_alt = df_model[['log_price', 'days', 'Private room', 'Shared room', 'instant_bookable_dummy']].copy()\nX_alt.insert(0, 'intercept', 1)\nX_alt_matrix = X_alt.values\nX_alt_columns = X_alt.columns.tolist()\n\n# Define outcome\ny_alt = df_model['number_of_reviews'].values\n\n# Fit model\ninitial_beta_alt = np.zeros(X_alt_matrix.shape[1])\nresult_alt = minimize(poisson_regression_log_likelihood, x0=initial_beta_alt,\n                      args=(y_alt, X_alt_matrix), method='BFGS')\n\n# Extract results\nbeta_alt = result_alt.x\nvcov_alt = result_alt.hess_inv if isinstance(result_alt.hess_inv, np.ndarray) else result_alt.hess_inv.todense()\nse_alt = np.sqrt(np.diag(vcov_alt))\n\n# Create table\nsummary_alt = pd.DataFrame({\n    'Coefficient': beta_alt,\n    'Std. Error': se_alt\n}, index=X_alt_columns).round(4)\n\nsummary_alt\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\n\n\n\n\nintercept\n2.4058\n0.0137\n\n\nlog_price\n0.1018\n0.0026\n\n\ndays\n0.0001\n0.0000\n\n\nPrivate room\n0.0570\n0.0033\n\n\nShared room\n-0.1355\n0.0093\n\n\ninstant_bookable_dummy\n0.3598\n0.0031\n\n\n\n\n\n\n\nThis reduced model captures the influence of pricing, availability, and room type on expected review counts, while omitting subjective post-stay feedback.\nThe table above reports the coefficients from a reduced Poisson regression model using only key listing characteristics that are known prior to guest interaction. These include price (log-transformed), room type, number of days listed, and whether the listing is instantly bookable.\nWe can find that:\n\nlog_price (0.1018): Listings with higher prices are associated with more reviews, though the effect is slightly smaller than in the full model. This suggests price still plays a role even when review-based variables are excluded.\ndays (0.0001): Longer-listed properties receive more reviews, as expected. The effect is small per day but compounds over time.\nRoom Type:\n\nPrivate room remains positive and significant, suggesting some demand for more affordable options.\nShared room has a stronger negative effect (−0.1355) than in the full model, likely because this model does not control for review-based user experiences.\n\ninstant_bookable_dummy (0.3598): The effect of instant booking is even stronger in this reduced model. Exponentiating the coefficient:\n\\(\\exp(0.3598) \\approx 1.43\\), meaning listings with instant booking enabled receive 43% more reviews than those without.\n\nOverall, this simpler model performs well and confirms the robustness of key insights from the full model. It emphasizes that hosts can meaningfully influence booking performance through pricing strategy, room configuration, and booking settings — all of which are controllable pre-listing factors."
  }
]